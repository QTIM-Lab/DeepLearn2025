{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This script trains linear/logistic regression models to predict demographics from the RETFound features.\n",
    "\n",
    "Steps:\n",
    "1) Load the RETFound features\n",
    "2) Load the demographics data\n",
    "3) Split the data into training and testing sets\n",
    "4) Train a linear/logistic regression model\n",
    "5) Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the RETFound features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(folder: str) -> pd.DataFrame:\n",
    "    \"\"\"Returns a dataframe with features\"\"\"\n",
    "    features: list = []\n",
    "    for file in [f for f in os.listdir(folder) if os.path.splitext(f)[1] == '.pkl']:\n",
    "        file = os.path.join(folder, file)\n",
    "        with open(file, 'rb') as f_in:\n",
    "            features += pk.load(f_in)\n",
    "    print(f'Read features for {len(features):,d} images')\n",
    "\n",
    "    df_features: pd.DataFrame = pd.DataFrame(features)\n",
    "    df_features['filename'] = df_features['path'].apply(os.path.basename)\n",
    "    df_features = df_features.set_index('filename')\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features for 7,000 images\n",
      "Loaded 7,000 features\n"
     ]
    }
   ],
   "source": [
    "FEATURES_FOLDER = '../features'\n",
    "\n",
    "# load features and split them\n",
    "df_features = get_features(FEATURES_FOLDER)\n",
    "print(f'Loaded {len(df_features):,d} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "METADATA_FILE: str = '../full_df.csv'\n",
    "\n",
    "AGE_COLUMN = 'Patient Age'\n",
    "GENDER_COLUMN = 'Patient Sex'\n",
    "LATERALITY_COLUMN = 'Laterality'\n",
    "TARGET_COLUMN = GENDER_COLUMN\n",
    "\n",
    "# split percentages\n",
    "TEST_SIZE = 0.2 # test is 20%\n",
    "VAL_SIZE = 0.2 / (1 - TEST_SIZE) # val is 20% of overall dataset, so it's (20/80)% of the train+val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata and preprocessing data...\n",
      "Any missing values in target columns:\n",
      "Patient Age    False\n",
      "Patient Sex    False\n",
      "dtype: bool\n",
      "Gender and age distributions:\n",
      "       Patient Age  Patient Sex\n",
      "count  6392.000000  6392.000000\n",
      "mean     57.857947     0.464330\n",
      "std      11.727737     0.498765\n",
      "min       1.000000     0.000000\n",
      "25%      51.000000     0.000000\n",
      "50%      59.000000     0.000000\n",
      "75%      66.000000     1.000000\n",
      "max      91.000000     1.000000\n",
      "Number and percentage of patients in each split\n",
      "      filename           \n",
      "         count percentage\n",
      "split                    \n",
      "test      1279   0.200094\n",
      "train     3834   0.599812\n",
      "val       1279   0.200094\n",
      "Number of images in each split\n",
      "Train: 7,668\n",
      "Test: 2,558\n",
      "Val: 2,558\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading metadata and preprocessing data...\")\n",
    "df = pd.read_csv(METADATA_FILE)\n",
    "\n",
    "# create a column with both file paths (left and right) - will be useful later\n",
    "df['filenames'] = df.apply(lambda x: x[['Left-Fundus', 'Right-Fundus']].values.tolist(), axis=1)\n",
    "\n",
    "# Handle the target column\n",
    "# Convert age to numeric\n",
    "df[AGE_COLUMN] = pd.to_numeric(df[AGE_COLUMN], errors='coerce')\n",
    "\n",
    "# Map gender to 0/1\n",
    "df[GENDER_COLUMN] = df[GENDER_COLUMN].map({'Male': 0, 'Female': 1})\n",
    "df[GENDER_COLUMN] = df[GENDER_COLUMN].astype(int)\n",
    "\n",
    "# check that there are no missing values\n",
    "print('Any missing values in target columns:')\n",
    "print(df[[AGE_COLUMN, GENDER_COLUMN]].isna().any())\n",
    "\n",
    "print(f\"Gender and age distributions:\")\n",
    "print(df[[AGE_COLUMN, GENDER_COLUMN]].describe())\n",
    "\n",
    "# generate train/test/val (60:20:20) splits at patient level (the dataframe has one row per patient). Set random_state for reproducibility\n",
    "trainval, test = train_test_split(df, test_size=TEST_SIZE, random_state=42)\n",
    "train, val = train_test_split(trainval, test_size=VAL_SIZE, random_state=42)\n",
    "\n",
    "# check that the splits were done correctly and that we obtained the intended patient-level ratios\n",
    "df['split'] = 'train'\n",
    "df.loc[test.index, 'split'] = 'test'\n",
    "df.loc[val.index, 'split'] = 'val'\n",
    "\n",
    "# sanity check: all patients selected for training are actually in the training split\n",
    "assert (df['split'].iloc[train.index].eq('train')).all(), 'Train split is not correct'\n",
    "\n",
    "print('Number and percentage of patients in each split')\n",
    "print(df.groupby('split').agg({'filename': ['count', ('percentage', lambda x: x.count()/len(df))]}))\n",
    "\n",
    "# go from dataframes at patient level to dataframes at image level\n",
    "train = train.explode('filenames')\n",
    "test = test.explode('filenames')\n",
    "val = val.explode('filenames')\n",
    "trainval = trainval.explode('filenames')\n",
    "\n",
    "# add laterality column based on the filename Right = 1\n",
    "train[LATERALITY_COLUMN] = train['filenames'].apply(lambda x: int('right' in x))\n",
    "test[LATERALITY_COLUMN] = test['filenames'].apply(lambda x: int('right' in x))\n",
    "val[LATERALITY_COLUMN] = val['filenames'].apply(lambda x: int('right' in x))\n",
    "\n",
    "\n",
    "# after this opearation, filenames will store the name of the image the dataframe row refers to\n",
    "print('Number of images in each split')\n",
    "print(f'Train: {len(train):,d}')\n",
    "print(f'Test: {len(test):,d}')\n",
    "print(f'Val: {len(val):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together the imaging features for training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need val to run a GridSearch - sklearn wants train and val together + indices of the train and val splits\n",
    "trainval_X = np.stack(df_features.loc[trainval['filenames']]['features'].values)\n",
    "trainval_y = trainval[TARGET_COLUMN]\n",
    "train_idx = np.arange(len(trainval_X))[trainval.index.isin(train.index)]\n",
    "val_idx = np.arange(len(trainval_X))[trainval.index.isin(val.index)]\n",
    "\n",
    "assert train_idx.shape[0] == len(train)\n",
    "assert val_idx.shape[0] == len(val)\n",
    "\n",
    "train_X = np.stack(df_features.loc[train['filenames']]['features'].values)\n",
    "train_y = train[TARGET_COLUMN]\n",
    "\n",
    "val_X = np.stack(df_features.loc[val['filenames']]['features'].values)\n",
    "val_y = val[TARGET_COLUMN]\n",
    "\n",
    "test_X = np.stack(df_features.loc[test['filenames']]['features'].values)\n",
    "test_y = test[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear/logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "MAX_ITER = 1000\n",
    "\n",
    "RESULTS_FOLDER = f'../results/{TARGET_COLUMN}/retfound_models'\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(task: str, args: dict = {}):\n",
    "    \"\"\"Returns the appropriate model to train for the given task\"\"\"\n",
    "\n",
    "    if task == AGE_COLUMN:\n",
    "        return LinearRegression()\n",
    "    \n",
    "    return LogisticRegression(class_weight='balanced', max_iter=MAX_ITER, solver='saga', verbose=True, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# add hyperparameter search on val set\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TARGET_COLUMN \u001b[38;5;241m!=\u001b[39m AGE_COLUMN:\n\u001b[0;32m----> 3\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\u001b[43mget_model\u001b[49m(TARGET_COLUMN), \n\u001b[1;32m      4\u001b[0m                             param_grid\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpenalty\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m)}, \n\u001b[1;32m      5\u001b[0m                             scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m[(train_idx, val_idx)])\n\u001b[1;32m      6\u001b[0m     grid_search\u001b[38;5;241m.\u001b[39mfit(trainval_X, trainval_y)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(grid_search\u001b[38;5;241m.\u001b[39mcv_results_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model' is not defined"
     ]
    }
   ],
   "source": [
    "# add hyperparameter search on val set\n",
    "if TARGET_COLUMN != AGE_COLUMN:\n",
    "    grid_search = GridSearchCV(get_model(TARGET_COLUMN), \n",
    "                            param_grid={'C': (1, 10), 'penalty': ('l1', 'l2')}, \n",
    "                            scoring='roc_auc', cv=[(train_idx, val_idx)])\n",
    "    grid_search.fit(trainval_X, trainval_y)\n",
    "\n",
    "    print(grid_search.cv_results_)\n",
    "    with open(os.path.join(RESULTS_FOLDER, 'model_pars.pk'), 'wb') as f:\n",
    "        pk.dump(grid_search, f)\n",
    "\n",
    "    clf = get_model(TARGET_COLUMN, grid_search.best_params_).fit(train_X, train_y)\n",
    "else:\n",
    "    clf = get_model(TARGET_COLUMN).fit(train_X, train_y)\n",
    "\n",
    "# save model\n",
    "with open(os.path.join(RESULTS_FOLDER, 'model.pk'), 'wb') as f:\n",
    "    pk.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference only: load the model and make predictions on the test set\n",
    "\n",
    "import pickle as pk\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the model\n",
    "with open(os.path.join(RESULTS_FOLDER, 'model.pk'), 'rb') as f:\n",
    "    clf = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6489444878811571, 'auc': 0.7123379877563454}\n",
      "array([[922, 444],\n",
      "       [454, 738]])\n"
     ]
    }
   ],
   "source": [
    "if TARGET_COLUMN == AGE_COLUMN:\n",
    "    test_labels = clf.predict(test_X)\n",
    "    test['pred'] = test_labels\n",
    "else:\n",
    "    test_prob = clf.predict_proba(test_X)[:,1]\n",
    "    test_labels = np.where(test_prob > 0.5, np.ones_like(test_prob), np.zeros_like(test_prob))\n",
    "\n",
    "    # save test predictions\n",
    "    test['pred_prob'] = test_prob\n",
    "\n",
    "test.to_csv(os.path.join(RESULTS_FOLDER, 'test_preds.csv'), index=False)\n",
    "\n",
    "# compute evaluation metrics\n",
    "metrics: dict = {}\n",
    "if TARGET_COLUMN == AGE_COLUMN:\n",
    "    metrics['r2'] = r2_score(test_y, test_labels)\n",
    "    metrics['mae'] = mean_absolute_error(test_y, test_labels)\n",
    "    # scatter plot of true vs predicted\n",
    "    plt.scatter(test_y, test_labels)\n",
    "    plt.xlabel('True Age')\n",
    "    plt.ylabel('Predicted Age')\n",
    "    plt.title('True vs Predicted Age')\n",
    "    # also plot the learned line\n",
    "    plt.plot(test_y, test_y, color='red')\n",
    "    plt.show()\n",
    "    confusion = None\n",
    "else:\n",
    "    metrics['accuracy'] = accuracy_score(test_y, test_labels)\n",
    "    confusion = confusion_matrix(test_y, test_labels)\n",
    "    metrics['auc'] = roc_auc_score(test_y, test_prob)\n",
    "\n",
    "pprint(metrics)\n",
    "pprint(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt_codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
